{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3F1PLUPHYd4h9faYxNSA5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSITHRAM/Interactive-Online-Class/blob/main/Copy_of_emotion_detection_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/googlefonts/roboto/raw/main/src/hinted/Roboto-Bold.ttf -O /content/bold.ttf\n",
        "# Download the Roboto-Bold.ttf font and save it as bold.ttf\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "!apt-get update\n",
        "!apt-get install -y libfreetype6-dev libpng-dev\n",
        "!pip install -U Pillow\n",
        "!pip install deepface\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "from deepface import DeepFace\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "\n",
        "# Function to convert JavaScript object into OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    print(\"js_to_image called\")  # Debugging print\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    print(\"Image shape:\", img.shape)  # Debugging print\n",
        "    return img\n",
        "\n",
        "\n",
        "# Function to overlay text on an image\n",
        "def text_to_bytes(bbox_array, text=\"Vision Programmer\", text_position=(150, 275), font_size=50):\n",
        "    print(\"text_to_bytes called\")  # Debugging print\n",
        "    if bbox_array.shape != (480, 640, 4):\n",
        "        raise ValueError(\"bbox_array must have the shape (480, 640, 4)\")\n",
        "\n",
        "    img = Image.fromarray(bbox_array, 'RGBA')\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    try:\n",
        "        font_path = '/content/bold.ttf'  # Path where the font is downloaded - you might need to adjust this\n",
        "        font = ImageFont.truetype(font_path, font_size)\n",
        "    except IOError:\n",
        "        raise IOError(\"Font file not found. Ensure the font is downloaded correctly.\")\n",
        "\n",
        "    draw.text(text_position, text, fill=(0, 0, 255, 255), font=font)\n",
        "\n",
        "    iobuf = io.BytesIO()\n",
        "    img.save(iobuf, format='png')\n",
        "\n",
        "    img_bytes = 'data:image/png;base64,{}'.format(\n",
        "        b64encode(iobuf.getvalue()).decode('utf-8')\n",
        "    )\n",
        "\n",
        "    print(\"Overlay image generated\")  # Debugging print\n",
        "    return img_bytes\n",
        "\n",
        "\n",
        "# Function to display the video stream\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "        async function stream_frame(label, bbox) {\n",
        "            const video = document.createElement('video');\n",
        "            const canvas = document.createElement('canvas');\n",
        "            const ctx = canvas.getContext('2d');\n",
        "\n",
        "            // Get access to the user's camera\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Set canvas dimensions\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "\n",
        "            // Draw video frame and overlay\n",
        "            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "\n",
        "            // Convert canvas content to data URL\n",
        "            const img = canvas.toDataURL('image/jpeg');\n",
        "\n",
        "            // Return the data\n",
        "            return { img };\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    print(\"Video stream started\")  # Debugging print\n",
        "\n",
        "\n",
        "# Function to get a video frame\n",
        "def video_frame(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    print(\"Video frame received\")  # Debugging print\n",
        "    return data\n",
        "\n",
        "\n",
        "# Main execution loop\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    img0 = js_to_image(js_reply[\"img\"])\n",
        "    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        print(\"Analyzing emotion...\")  # Debugging print\n",
        "        # Setting enforce_detection to False to avoid the ValueError if no face is detected\n",
        "        objs = DeepFace.analyze(img_path=img0, actions=['emotion'], enforce_detection=False)\n",
        "        emotion = objs[0]['dominant_emotion']\n",
        "        print(\"Detected emotion:\", emotion)  # Debugging print\n",
        "        base64 = text_to_bytes(bbox_array, text=emotion)\n",
        "        bbox = base64\n",
        "    except Exception as e:\n",
        "        print(f\"Error during emotion analysis: {e}\")\n",
        "        base64 = text_to_bytes(bbox_array)\n",
        "        bbox = base64"
      ],
      "metadata": {
        "id": "x-X2jdTqyaC4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c30cc993-0af9-413b-9e26-4d81f47d053a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 16:16:07--  https://github.com/googlefonts/roboto/raw/main/src/hinted/Roboto-Bold.ttf\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/googlefonts/roboto-2/raw/main/src/hinted/Roboto-Bold.ttf [following]\n",
            "--2025-03-02 16:16:07--  https://github.com/googlefonts/roboto-2/raw/main/src/hinted/Roboto-Bold.ttf\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/googlefonts/roboto-2/main/src/hinted/Roboto-Bold.ttf [following]\n",
            "--2025-03-02 16:16:07--  https://raw.githubusercontent.com/googlefonts/roboto-2/main/src/hinted/Roboto-Bold.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 514260 (502K) [application/octet-stream]\n",
            "Saving to: ‘/content/bold.ttf’\n",
            "\n",
            "/content/bold.ttf   100%[===================>] 502.21K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-02 16:16:08 (24.8 MB/s) - ‘/content/bold.ttf’ saved [514260/514260]\n",
            "\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Fetched 257 kB in 2s (136 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libpng-dev is already the newest version (1.6.37-3build5).\n",
            "libfreetype6-dev is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: deepface in /usr/local/lib/python3.11/dist-packages (0.0.93)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Requirement already satisfied: flask-cors>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.0.1)\n",
            "Requirement already satisfied: mtcnn>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.0.0)\n",
            "Requirement already satisfied: retina-face>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.0.17)\n",
            "Requirement already satisfied: fire>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.7.0)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (23.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (4.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        async function stream_frame(label, bbox) {\n",
              "            const video = document.createElement('video');\n",
              "            const canvas = document.createElement('canvas');\n",
              "            const ctx = canvas.getContext('2d');\n",
              "\n",
              "            // Get access to the user's camera\n",
              "            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "\n",
              "            // Set canvas dimensions\n",
              "            canvas.width = video.videoWidth;\n",
              "            canvas.height = video.videoHeight;\n",
              "\n",
              "            // Draw video frame and overlay\n",
              "            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
              "\n",
              "            // Convert canvas content to data URL\n",
              "            const img = canvas.toDataURL('image/jpeg');\n",
              "\n",
              "            // Return the data\n",
              "            return { img };\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video stream started\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: happy\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: happy\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: happy\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: happy\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: happy\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: sad\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n",
            "Video frame received\n",
            "js_to_image called\n",
            "Image shape: (480, 640, 3)\n",
            "Analyzing emotion...\n",
            "Detected emotion: neutral\n",
            "text_to_bytes called\n",
            "Overlay image generated\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4e4662437eda>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mjs_reply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mjs_reply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4e4662437eda>\u001b[0m in \u001b[0;36mvideo_frame\u001b[0;34m(label, bbox)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Function to get a video frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvideo_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream_frame(\"{}\", \"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Video frame received\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}